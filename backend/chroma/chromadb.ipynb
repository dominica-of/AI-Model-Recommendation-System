{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b86fc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "import pandas as pd\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07776c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start server\n",
    "# docker run -d -p 8000:8000 --name chroma-server -v chroma_db:/chroma/chroma chromadb/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5d75b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746093344848437467"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "client.heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57da9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(name=\"model_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42f98488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=model_data)]\n"
     ]
    }
   ],
   "source": [
    "collections = client.list_collections()\n",
    "print(collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping row 197: missing Model ID\n",
      "⚠️ Skipping row 199: missing Model ID\n",
      "⚠️ Skipping row 200: missing Model ID\n",
      "⚠️ Skipping row 201: missing Model ID\n",
      "✅ Data saved to ChromaDB. Skipped 4 rows with missing IDs.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_metadata_chromadb.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m client = chromadb.HttpClient(host=\u001b[33m'\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m'\u001b[39m, port=\u001b[32m8000\u001b[39m)\n\u001b[32m     24\u001b[39m collection = client.get_or_create_collection(name=\u001b[33m'\u001b[39m\u001b[33mmodel_data\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m df_chroma = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_metadata_chromadb.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m embedder = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     29\u001b[39m embeddings = embedder.encode(df_chroma[\u001b[33m'\u001b[39m\u001b[33mcomposite_text\u001b[39m\u001b[33m'\u001b[39m].tolist(), show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_metadata_chromadb.csv'"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = client.get_or_create_collection(name='model_data')\n",
    "\n",
    "df_chroma = pd.read_csv('model_metadata_chromadb.csv')\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = embedder.encode(df_chroma['composite_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "skipped = 0\n",
    "for idx, row in df_chroma.iterrows():\n",
    "    model_id = row['Model']\n",
    "    if pd.isna(model_id):\n",
    "        print(f\"⚠️ Skipping row {idx}: missing Model ID\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "    collection.add(\n",
    "        ids=str(model_id),\n",
    "        documents=row['composite_text'],\n",
    "        embeddings=embeddings[idx]\n",
    "    )\n",
    "\n",
    "print(f\"Data saved to ChromaDB. Skipped {skipped} rows with missing IDs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "163cc39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: ['Bidirectional-LSTM', 'BERT', 'Flair', 'Sequence_tagging', 'gluon_nlp', 'tf_ner', 'ERNIE', 'nvidia/NV-Embed-v2', 'intfloat/multilingual-e5-large', 'canine-s', 'Transformer XL', 'zh-ner-tf', 'NCRF', 'targer', 'rnnmorph', 'BERT_Tensorflow', 'RoBERTa', 'roberta-base-squad-2', 'BERT_DeepPavlov', 'Transformers', 'ParlAI', 'haystack', 'distilbert/distilbert-base-cased-distilled-squad', 'deepset/bert-large-uncased-whole-word-masking-squad2', 'Seq2seq with Attention', 'Fairseq', 'Pegasus', 'BigBird', 'Pointer-Generator network', 'tensor2tensor', 'PaddleNLP', 'Bert2Bert Small ', 'facebook/bart-large-cnn', 'google/pegasus-cnn_dailymail', 'google/pegasus-xsum', 'TextCNN', 'Universal Language Model Fine-tuning for Text Classification', 'Convolutional Neural Networks for Sentence Classification', 'pyText', 'mt-dnn', 'DeBERTa', 'Graph Attention Networks_Tensorflow', 'Hierarchical Attention Networks for Document Classification', 'Graph Attention Networks_PyTorch', 'Graph Convolutional Networks', 'Specter', 'DocBert', 'GloVe', 'SeqGAN', 'BART', 'Char-rnn-tensorflow', 'Unified Language Model Pre-Training', 'neuraltalk2', 'fairseq', 'deepseek-ai/DeepSeek-V3-0324', 'meta-llama/Meta-Llama-3-8B', 'deepseek-ai/DeepSeek-R1', 'google/byt5-base', 'google/flan-t5-xxl', 'facebook/m2m100_418M', 'sentence-transformers/all-MiniLM-L6-v2', 'sentence-transformers/all-mpnet-base-v2', 'BAAI/bge-m3', 'Attention is all you need', 'Tensor2tensor', 'PyTorch-Seq2Seq', 'nmt-keras', 'sockeye', 'marian', 'Tensorpack', 'PyTorch-GAN', 'CycleGAN', 'transformers', 'MUSE', 'facebook/mbart-large-50-many-to-many-mmt', 'Helsinki-NLP/opus-mt-de-en', 'google-t5/t5-base', 'distilbert-base-uncased-finetuned-sst-2-english', 'cardiffnlp/twitter-roberta-base-sentiment', 'FacebookAI/roberta-large-mnli', 'impira/layoutlm-document-qa', 'naver-clova-ix/donut-base-finetuned-docvqa', 'jinhybr/OCR-DocVQA-Donut', 'coqui/XTTS-v2', 'hexgrad/Kokoro-82M', 'suno/bark', 'demucs', '2Noise/ChatTTS', 'Wavenet', 'waveglow', 'Tacotron', 'TTS', 'PaddleSpeech', 'TensorflowTTS', 'Lightricks/LTX-Video', 'tencent/HunyuanVideo', 'THUDM/CogVideoX-5b', 'DeepSpeech', 'Listen, Attend and Spell', 'Neural Collaborative Filtering', 'DeepSpeech 2', 'Fairseq S2T', 'Qwen/Qwen2-Audio-7B-Instruct', 'fixie-ai/ultravox-v0_5-llama-3_2-1b', 'openai/whisper-large-v3-turbo', 'facebook/wav2vec2-base-960h', 'deepspeech', 'openai/whisper-large-v3', 'stabilityai/stable-audio-open-1.0', 'ConvTasNet', 'facebook/wav2vec2-large-robust', 'YAMnet', 'MIT/ast-finetuned-audioset-10-10-0.4593', 'opensmile', 'pyannote/segmentation-3.0', 'silvero vad', 'Qwen/Qwen2.5-VL-7B-Instruct', 'meta-llama/Llama-3.2-11B-Vision-Instruct', 'google/gemma-3-27b-it', 'Video-R1/Video-R1-7B', 'llava-hf/LLaVA-NeXT-Video-7B-hf', 'lmms-lab/LLaVA-Video-7B-Qwen2', 'SSD', 'Darknet', 'Faster-rcnn', 'Mask-RCNN', 'Detectron 2', 'pytorch-image-models', 'PaddleOCR', 'IDEA-Research/grounding-dino-base', 'google/owlvit-base-patch32', 'google/owlv2-large-patch14-ensemble', 'Mask R-CNN', 'alpha_pose_resnet101', 'Openpose', 'mmpose', 'DeepLabCut', 'AdelaiDet', 'MobileNetV2', 'EfficientNet', 'resnest269', 'ClassyVision', 'SqueezeNet', 'Super-gradients', 'vissl', 'timm/mobilenetv3_small_100.lamb_in1k', 'nvidia/MambaVision-L3-512-21K', 'google/vit-base-patch16-224', 'openai/clip-vit-large-patch14', 'google/siglip-so400m-patch14-384', 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224', 'Non-local neural networks', 'Grad-Cam++', 'slowfast', '3D-ResNets-PyTorch', 'PaddleDetection(non-local-neural-networks)', 'Towhee', 'scenic', 'facebook/sam-vit-huge', 'Zigeng/SlimSAM-uniform-77', 'facebook/sam2.1-hiera-large', 'Zhengyi/LLaMA-Mesh', 'openai/shap-e', 'ashawkey/LGM', 'TencentARC/InstantMesh', 'JeffreyXiang/TRELLIS-image-large', 'stabilityai/TripoSR', 'zongzhuofan/co-detr-vit-large-coco', 'briaai/RMBG-2.0', 'ZhengPeng7/BiRefNet_HR', 'Fully convolutional Networks for Semantic Segmentation', 'SegNet', 'deeplab_resnet_152_coco', 'Pytorch-semantic-segmentation', 'imgclsmob', 'dandelin/vilt-b32-finetuned-vqa', 'Salesforce/blip-vqa-base', 'openbmb/MiniCPM-Llama3-V-2_5-int4', 'segmentation-models', 'depth-anything/Depth-Anything-V2-Small-hf', 'tencent/DepthCrafter', 'Intel/dpt-hybrid-midas', 'Ultralytics/YOLOv8', 'facebook/detr-resnet-50', 'microsoft/table-transformer-detection', 'stable-diffusion-v1-5/stable-diffusion-v1-5', 'black-forest-labs/FLUX.1-dev', 'stabilityai/stable-diffusion-xl-refiner-1.0', 'lllyasviel/sd-controlnet-canny', 'stable-diffusion-v1-5/stable-diffusion-inpainting', 'Salesforce/blip-image-captioning-base', 'microsoft/trocr-base-handwritten', 'IAMJB/chexpert-mimic-cxr-findings-baseline', 'stabilityai/stable-video-diffusion-img2vid-xt', 'Wan-AI/Wan2.1-I2V-14B-480P-Diffusers', 'tencent/HunyuanVideo-I2V', 'xclid', 'videomaev2', 'vivit-b', 'google/vit-base-patch16-224-in21k', 'facebook/dinov2-base', 'nvidia/MambaVision-S-1K', 'Nothing for Descriptive Analysis', 'ShapeNet', 'Adversarial Autoencoders', 'Recommenders', 'mildnet', 'PaddleRec', 'FuxiCTR', 'ibm-granite/granite-timeseries-ttm-r1', 'Temporal Fusion Transformer (TFT)', 'LSTNet', 'darts', 'gluon-ts', 'qlib', 'DCRNN', 'n-beats', 'pytorch_geometric_temporal', 'tapas-base-finetuned-wtq', 'tapas-large-finetuned-wtq', 'tapex-large-finetuned-wtq', 'Efficient Estimation of Word Representations in Vector Space', 'DeepCTR', 'TabNet', 'XGBoost', 'CatBoost', 'LightGBM', 'Narrativaai/bloom-560m-finetuned-totto-table-to-text', 'Yale-LILY/reastap-large-finetuned-logicnlg', 'Multimodal Toolkit', 'ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8', 'Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B', 'ThomasSimonini/ppo-AntBulletEnv-v0', 'lerobot/pi0', 'nvidia/GR00T-N1-2B', 'physical-intelligence/fast', 'ecmwf/aifs-single-1.0', 'ibm-research/materials.pos-egnn', 'clefourrier/graphormer-base-pcqm4mv2', 'bztest/xlm-roberta-base-finetuned-panx-de', 'FacebookAI/xlm-roberta-large-finetuned-conll03-english', 'dslim/bert-base-NER', 'facebook/bart-large-mnli', 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli', 'joeddav/bart-large-mnli-yahoo-answers']\n",
      "Documents: ['Model: Bidirectional-LSTM. Task: Named Entity Recognition. Action:  Identify and classify named entities such as people, organizations, locations, and more. Input Data: Text. Framework: Tensorflow. Institution: Kamal Kraj', 'Model: BERT. Task: Named Entity Recognition. Action:  Identify and classify named entities such as people, organizations, locations, and more. Input Data: Text. Framework: Tensorflow. Institution: Google', 'Model: Flair. Task: Named Entity Recognition. Action:  Identify and classify named entities such as people, organizations, locations, and more. Input Data: Text. Framework: PyTorch. Institution: Flair', 'Model: Sequence_tagging. Task: Named Entity Recognition. Action:  Identify and classify named entities such as people, organizations, locations, and more. Input Data: Text. Framework: Tensorflow. Institution: Guillaume Genthial ', 'Model: gluon_nlp. Task: Named Entity Recognition. Action:  Identify and classify named entities such as people, organizations, locations, and more. Input Data: Text. Framework: mxnet. Institution: Gluon', 'Model: tf_ner. Task: Named Entity Recognition. Action:  Identify and classify named entities such as people, organizations, locations, and more. Input Data: Text. Framework: Tensorflow. Institution: Guillaume Genthial', 'Model: ERNIE. Task: Named Entity Recognition. Action:  Identify and classify named entities such as people, organizations, locations, and more. Input Data: Text. Framework: Paddle. Institution: n.a', 'Model: nvidia/NV-Embed-v2. Task: Feature Extraction. Action: . Input Data: Text. Framework: . Institution: ', 'Model: intfloat/multilingual-e5-large. Task: Feature Extraction. Action: . Input Data: Text. Framework: . Institution: ', 'Model: canine-s. Task: Feature Extraction. Action: . Input Data: Text. Framework: . Institution: google', 'Model: Transformer XL. Task: Part-of-Speech(POS) tagging. Action: assign a part-of-speech tag (e.g., noun, verb, adjective) to each word in the text.. Input Data: Text. Framework: Tensorflow. Institution: Kimi Young', 'Model: zh-ner-tf. Task: Part-of-Speech(POS) tagging. Action: assign a part-of-speech tag (e.g., noun, verb, adjective) to each word in the text.. Input Data: Text. Framework: Tensorflow. Institution: Penghui Wei', 'Model: NCRF. Task: Part-of-Speech(POS) tagging. Action: assign a part-of-speech tag (e.g., noun, verb, adjective) to each word in the text.. Input Data: Text. Framework: PyTorch. Institution: Jie Yang', 'Model: targer. Task: Part-of-Speech(POS) tagging. Action: assign a part-of-speech tag (e.g., noun, verb, adjective) to each word in the text.. Input Data: Text. Framework: PyTorch. Institution: Artem Chernodub', 'Model: rnnmorph. Task: Part-of-Speech(POS) tagging. Action: assign a part-of-speech tag (e.g., noun, verb, adjective) to each word in the text.. Input Data: Text. Framework: ?. Institution: Ilya Gusev', 'Model: BERT_Tensorflow. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: Tensorflow. Institution: Google', 'Model: RoBERTa. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: PyTorch. Institution: Facebook', 'Model: roberta-base-squad-2. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: Haystack. Institution: deepset ai', 'Model: BERT_DeepPavlov. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: DeepPavlov. Institution: DeepPavlov lab', 'Model: Transformers. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: Paddle. Institution: Paddle', 'Model: ParlAI. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: PyTorch. Institution: Facebook', 'Model: haystack. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: PyTorch. Institution: deepset ai', 'Model: distilbert/distilbert-base-cased-distilled-squad. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: . Institution: ', 'Model: deepset/bert-large-uncased-whole-word-masking-squad2. Task: Question Answering. Action: provide answers to questions based on the input data.. Input Data: Text. Framework: . Institution: ', 'Model: Seq2seq with Attention. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: Tensorflow. Institution: Google', 'Model: Fairseq. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: PyTorch. Institution: Facebook', 'Model: Pegasus. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: Tensorflow. Institution: Google', 'Model: BigBird. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: Tensorflow. Institution: Google', 'Model: Pointer-Generator network. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: Tensorflow. Institution: Abi See', 'Model: tensor2tensor. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: Tensorflow. Institution: n.a', 'Model: PaddleNLP. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: Paddle. Institution: n.a', 'Model: Bert2Bert Small . Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: Pytorch. Institution: ', 'Model: facebook/bart-large-cnn. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: . Institution: ', 'Model: google/pegasus-cnn_dailymail. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: . Institution: ', 'Model: google/pegasus-xsum. Task: Text summarization. Action:  Generate a concise summary of the input text.. Input Data: Text. Framework: . Institution: ', 'Model: TextCNN. Task: Sentiment Analysis. Action: determine the sentiment (positive, negative, or neutral) expressed in the text.. Input Data: Text. Framework: Gluon. Institution: n.a', 'Model: Universal Language Model Fine-tuning for Text Classification. Task: Sentiment Analysis. Action: determine the sentiment (positive, negative, or neutral) expressed in the text.. Input Data: Text. Framework: PyTorch. Institution: Fast ai', 'Model: Convolutional Neural Networks for Sentence Classification. Task: Sentiment Analysis. Action: determine the sentiment (positive, negative, or neutral) expressed in the text.. Input Data: Text. Framework: Paddle. Institution: Paddle', 'Model: pyText. Task: Sentiment Analysis. Action: determine the sentiment (positive, negative, or neutral) expressed in the text.. Input Data: Text. Framework: PyTorch. Institution: Facebook', 'Model: mt-dnn. Task: Sentiment Analysis. Action: determine the sentiment (positive, negative, or neutral) expressed in the text.. Input Data: Text. Framework: PyTorch. Institution: Namisan', 'Model: DeBERTa. Task: Sentiment Analysis. Action: determine the sentiment (positive, negative, or neutral) expressed in the text.. Input Data: Text. Framework: PyTorch. Institution: Microsoft', 'Model: Graph Attention Networks_Tensorflow. Task: Document Classification. Action: assign a category or class to the input document based on its content.. Input Data: Text. Framework: Tensorflow. Institution: Petar Veličković', 'Model: Hierarchical Attention Networks for Document Classification. Task: Document Classification. Action: assign a category or class to the input document based on its content.. Input Data: Text. Framework: PyTorch. Institution: EDGE', 'Model: Graph Attention Networks_PyTorch. Task: Document Classification. Action: assign a category or class to the input document based on its content.. Input Data: Text. Framework: PyTorch. Institution: Diego Antognini', 'Model: Graph Convolutional Networks. Task: Document Classification. Action: assign a category or class to the input document based on its content.. Input Data: Text. Framework: Tensorflow . Institution: Thomas Kipf', 'Model: Specter. Task: Document Classification. Action: assign a category or class to the input document based on its content.. Input Data: Text. Framework: PyTorch. Institution: Allen AI', 'Model: DocBert. Task: Document Classification. Action: assign a category or class to the input document based on its content.. Input Data: Text. Framework: PyTorch. Institution: Castorini', 'Model: GloVe. Task: Document Classification. Action: assign a category or class to the input document based on its content.. Input Data: Text. Framework: ?. Institution: Stanford', 'Model: SeqGAN. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: Tensorflow. Institution: Lantao Yu', 'Model: BART. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: PyTorch. Institution: Huggingface', 'Model: Char-rnn-tensorflow. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: Tensorflow. Institution: Sherjil Ozair', 'Model: Unified Language Model Pre-Training. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: PyTorch. Institution: Microsoft', 'Model: neuraltalk2. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: PyTorch. Institution: Karpathy', 'Model: fairseq. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: PyTorch. Institution: Facebook', 'Model: deepseek-ai/DeepSeek-V3-0324. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: . Institution: ', 'Model: meta-llama/Meta-Llama-3-8B. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: . Institution: ', 'Model: deepseek-ai/DeepSeek-R1. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: . Institution: ', 'Model: google/byt5-base. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: . Institution: ', 'Model: google/flan-t5-xxl. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: . Institution: ', 'Model: facebook/m2m100_418M. Task: Text Generation. Action: generate new text based on the input, such as creative writing, article generation, or dialogue.. Input Data: Text. Framework: . Institution: ', 'Model: sentence-transformers/all-MiniLM-L6-v2. Task: Sentence Similarity. Action: . Input Data: Text. Framework: . Institution: ', 'Model: sentence-transformers/all-mpnet-base-v2. Task: Sentence Similarity. Action: . Input Data: Text. Framework: . Institution: ', 'Model: BAAI/bge-m3. Task: Sentence Similarity. Action: . Input Data: Text. Framework: . Institution: ', 'Model: Attention is all you need. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: Tensorflow. Institution: Google', 'Model: Tensor2tensor. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: Tensorflow. Institution: Tensorflow', 'Model: PyTorch-Seq2Seq. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: PyTorch. Institution: IBM', 'Model: nmt-keras. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: Tensorflow. Institution: Ivapeab', 'Model: sockeye. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: mxnet. Institution: Awslabs', 'Model: marian. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: ?. Institution: Marian-nmt', 'Model: Tensorpack. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: Tensorflow. Institution: Google', 'Model: PyTorch-GAN. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: PyTorch. Institution: Erik Linder-Noren', 'Model: CycleGAN. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: Tensorflow. Institution: Jun-Yan Zhu', 'Model: transformers. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: PyTorch. Institution: HuggingFace', 'Model: MUSE. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: PyTorch. Institution: Facebook', 'Model: facebook/mbart-large-50-many-to-many-mmt. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: . Institution: ', 'Model: Helsinki-NLP/opus-mt-de-en. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: . Institution: ', 'Model: google-t5/t5-base. Task: Machine translation. Action: translate the input text into another target language.. Input Data: Text. Framework: . Institution: ', 'Model: distilbert-base-uncased-finetuned-sst-2-english. Task: Text Classification. Action: . Input Data: Text. Framework: . Institution: ', 'Model: cardiffnlp/twitter-roberta-base-sentiment. Task: Text Classification. Action: . Input Data: Text. Framework: . Institution: ', 'Model: FacebookAI/roberta-large-mnli. Task: Text Classification. Action: . Input Data: Text. Framework: . Institution: ', 'Model: impira/layoutlm-document-qa. Task: Document Question Answering. Action: . Input Data: Text. Framework: . Institution: ', 'Model: naver-clova-ix/donut-base-finetuned-docvqa. Task: Document Question Answering. Action: . Input Data: Text. Framework: . Institution: ', 'Model: jinhybr/OCR-DocVQA-Donut. Task: Document Question Answering. Action: . Input Data: Text. Framework: . Institution: ', 'Model: coqui/XTTS-v2. Task: Text-to-Speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: . Institution: ', 'Model: hexgrad/Kokoro-82M. Task: Text-to-Speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: . Institution: ', 'Model: suno/bark. Task: Text-to-Speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: . Institution: ', 'Model: demucs. Task: Text-to-Speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: . Institution: ', 'Model: 2Noise/ChatTTS. Task: Text-to-Speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: . Institution: ', 'Model: Wavenet. Task: Text to speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: Tensorflow. Institution: n.a', 'Model: waveglow. Task: Text to speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: PyTorch. Institution: n.a', 'Model: Tacotron. Task: Text to speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: Tensorflow. Institution: Corentin Jemine', 'Model: TTS. Task: Text to speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: PyTorch. Institution: Coqui-ai', 'Model: PaddleSpeech. Task: Text to speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: Paddle. Institution: Paddle', 'Model: TensorflowTTS. Task: Text to speech. Action: convert the written text into spoken audio.. Input Data: Text. Framework: Tensorflow. Institution: as-ideas', 'Model: Lightricks/LTX-Video. Task: Text-to-Video. Action: convert the written text into video.. Input Data: Text. Framework: . Institution: ', 'Model: tencent/HunyuanVideo. Task: Text-to-Video. Action: convert the written text into video.. Input Data: Text. Framework: . Institution: ', 'Model: THUDM/CogVideoX-5b. Task: Text-to-Video. Action: convert the written text into video.. Input Data: Text. Framework: . Institution: ', 'Model: DeepSpeech. Task: Speech to text. Action: convert the spoken words into text.. Input Data: Audio. Framework: Tensorflow. Institution: n.a', 'Model: Listen, Attend and Spell. Task: Speech to text. Action: convert the spoken words into text.. Input Data: Audio. Framework: PyTorch. Institution: n.a', 'Model: Neural Collaborative Filtering. Task: Speech to text. Action: convert the spoken words into text.. Input Data: Audio. Framework: Tensorflow. Institution: n.a', 'Model: DeepSpeech 2. Task: Speech to text. Action: convert the spoken words into text.. Input Data: Audio. Framework: Paddle. Institution: n.a', 'Model: Fairseq S2T. Task: Speech to text. Action: convert the spoken words into text.. Input Data: Audio. Framework: PyTorch. Institution: Facebook', 'Model: Qwen/Qwen2-Audio-7B-Instruct. Task: Speech to text. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: fixie-ai/ultravox-v0_5-llama-3_2-1b. Task: Speech to text. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: openai/whisper-large-v3-turbo. Task: Speech to text. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: facebook/wav2vec2-base-960h. Task: Automatic speech Recognition. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: deepspeech. Task: Automatic speech Recognition. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: openai/whisper-large-v3. Task: Automatic speech Recognition. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: stabilityai/stable-audio-open-1.0. Task: Audio-to-Audio. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: ConvTasNet. Task: Audio-to-Audio. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: facebook/wav2vec2-large-robust. Task: Audio Classification. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: YAMnet. Task: Audio Classification. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: MIT/ast-finetuned-audioset-10-10-0.4593. Task: Audio Classification. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: opensmile. Task: Voice Activity Detection. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: pyannote/segmentation-3.0. Task: Voice Activity Detection. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: silvero vad. Task: Voice Activity Detection. Action: . Input Data: Audio. Framework: . Institution: ', 'Model: Qwen/Qwen2.5-VL-7B-Instruct. Task: Image to Text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: meta-llama/Llama-3.2-11B-Vision-Instruct. Task: Image to Text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: google/gemma-3-27b-it. Task: Image to Text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Video-R1/Video-R1-7B. Task: Video to Text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: llava-hf/LLaVA-NeXT-Video-7B-hf. Task: Video to Text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: lmms-lab/LLaVA-Video-7B-Qwen2. Task: Video to Text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: SSD. Task: Object detection. Action: detect and localize objects of interest within the input.. Input Data: Video/Images. Framework: MXNet. Institution: n.a', 'Model: Darknet. Task: Object detection. Action: detect and localize objects of interest within the input.. Input Data: Video/Images. Framework: Tensorflow. Institution: Alexey', 'Model: Faster-rcnn. Task: Object detection. Action: detect and localize objects of interest within the input.. Input Data: Video/Images. Framework: PyTorch. Institution: n.a', 'Model: Mask-RCNN. Task: Object detection. Action: detect and localize objects of interest within the input.. Input Data: Video/Images. Framework: Tensorflow. Institution: MatterPort', 'Model: Detectron 2. Task: Object detection. Action: detect and localize objects of interest within the input.. Input Data: Video/Images. Framework: PyTorch. Institution: Facebook', 'Model: pytorch-image-models. Task: Object detection. Action: detect and localize objects of interest within the input.. Input Data: Video/Images. Framework: PyTorch. Institution: rwightman', 'Model: PaddleOCR. Task: Object detection. Action: detect and localize objects of interest within the input.. Input Data: Video/Images. Framework: Paddle. Institution: Paddle', 'Model: IDEA-Research/grounding-dino-base. Task: Zero-shot Object Detection. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: google/owlvit-base-patch32. Task: Zero-shot Object Detection. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: google/owlv2-large-patch14-ensemble. Task: Zero-shot Object Detection. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Mask R-CNN. Task: Pose estimation. Action: estimate the pose or body positioning of people or objects within the input.. Input Data: Video/Images. Framework: Tensorflow. Institution: n.a', 'Model: alpha_pose_resnet101. Task: Pose estimation. Action: estimate the pose or body positioning of people or objects within the input.. Input Data: Video/Images. Framework: MXNet. Institution: Hao-Shu FANG', 'Model: Openpose. Task: Pose estimation. Action: estimate the pose or body positioning of people or objects within the input.. Input Data: Video/Images. Framework: PyTorch. Institution: n.a', 'Model: mmpose. Task: Pose estimation. Action: estimate the pose or body positioning of people or objects within the input.. Input Data: Video/Images. Framework: PyTorch. Institution: Open-mmlab', 'Model: DeepLabCut. Task: Pose estimation. Action: estimate the pose or body positioning of people or objects within the input.. Input Data: Video/Images. Framework: Tensorflow. Institution: DeepLabCut', 'Model: AdelaiDet. Task: Pose estimation. Action: estimate the pose or body positioning of people or objects within the input.. Input Data: Video/Images. Framework: PyTorch. Institution: n.a', 'Model: MobileNetV2. Task: Image classfication. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: MXNet. Institution: n.a', 'Model: EfficientNet. Task: Image classfication. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: Tensorflow. Institution: Google', 'Model: resnest269. Task: Image classfication. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: Gluon. Institution: n.a', 'Model: ClassyVision. Task: Image classfication. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: PyTorch. Institution: n.a', 'Model: SqueezeNet. Task: Image classfication. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: Caffe2. Institution: n.a', 'Model: Super-gradients. Task: Image classfication. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: PyTorch. Institution: Deci AI', 'Model: vissl. Task: Image classfication. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: PyTorch. Institution: Facebook', 'Model: timm/mobilenetv3_small_100.lamb_in1k. Task: Image Classification. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: . Institution: ', 'Model: nvidia/MambaVision-L3-512-21K. Task: Image Classification. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: . Institution: ', 'Model: google/vit-base-patch16-224. Task: Image Classification. Action: assign a label or category to the input image based on its content.. Input Data: Video/Images. Framework: . Institution: ', 'Model: openai/clip-vit-large-patch14. Task: Zero-shot Image Classification. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: google/siglip-so400m-patch14-384. Task: Zero-shot Image Classification. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224. Task: Zero-shot Image Classification. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Non-local neural networks. Task: Action recognition. Action: identify and classify the actions performed by people or objects in the input video.. Input Data: Video/Images. Framework: Caffe2. Institution: n.a', 'Model: Grad-Cam++. Task: Action recognition. Action: identify and classify the actions performed by people or objects in the input video.. Input Data: Video/Images. Framework: Tensorflow. Institution: n.a', 'Model: slowfast. Task: Action recognition. Action: identify and classify the actions performed by people or objects in the input video.. Input Data: Video/Images. Framework: Gluon. Institution: n.a', 'Model: 3D-ResNets-PyTorch. Task: Action recognition. Action: identify and classify the actions performed by people or objects in the input video.. Input Data: Video/Images. Framework: PyTorch. Institution: n.a', 'Model: PaddleDetection(non-local-neural-networks). Task: Action recognition. Action: identify and classify the actions performed by people or objects in the input video.. Input Data: Video/Images. Framework: Paddle. Institution: n.a', 'Model: Towhee. Task: Action recognition. Action: identify and classify the actions performed by people or objects in the input video.. Input Data: Video/Images. Framework: PyTorch. Institution: Towhee-io', 'Model: scenic. Task: Action recognition. Action: identify and classify the actions performed by people or objects in the input video.. Input Data: Video/Images. Framework: JAX. Institution: Google', 'Model: facebook/sam-vit-huge. Task: Mask Generation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Zigeng/SlimSAM-uniform-77. Task: Mask Generation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: facebook/sam2.1-hiera-large. Task: Mask Generation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Zhengyi/LLaMA-Mesh. Task: Text-to-3d. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: openai/shap-e. Task: Text-to-3d. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: ashawkey/LGM. Task: Text-to-3d. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: TencentARC/InstantMesh. Task: Image-to-3d. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: JeffreyXiang/TRELLIS-image-large. Task: Image-to-3d. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: stabilityai/TripoSR. Task: Image-to-3d. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: zongzhuofan/co-detr-vit-large-coco. Task: Image Segmentation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: briaai/RMBG-2.0. Task: Image Segmentation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: ZhengPeng7/BiRefNet_HR. Task: Image Segmentation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Fully convolutional Networks for Semantic Segmentation. Task: Semantic Segmentation. Action:  assign a semantic label to each pixel in the input image, effectively segmenting the image into meaningful regions or objects.. Input Data: Video/Images. Framework: Caffe2. Institution: n.a', 'Model: SegNet. Task: Semantic Segmentation. Action:  assign a semantic label to each pixel in the input image, effectively segmenting the image into meaningful regions or objects.. Input Data: Video/Images. Framework: MXNet. Institution: n.a', 'Model: deeplab_resnet_152_coco. Task: Semantic Segmentation. Action:  assign a semantic label to each pixel in the input image, effectively segmenting the image into meaningful regions or objects.. Input Data: Video/Images. Framework: Gluon. Institution: n.a', 'Model: Pytorch-semantic-segmentation. Task: Semantic Segmentation. Action:  assign a semantic label to each pixel in the input image, effectively segmenting the image into meaningful regions or objects.. Input Data: Video/Images. Framework: PyTorch. Institution: n.a', 'Model: imgclsmob. Task: Semantic Segmentation. Action:  assign a semantic label to each pixel in the input image, effectively segmenting the image into meaningful regions or objects.. Input Data: Video/Images. Framework: mxnet. Institution: n.a', 'Model: dandelin/vilt-b32-finetuned-vqa. Task: Visual Question Answering. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Salesforce/blip-vqa-base. Task: Visual Question Answering. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: openbmb/MiniCPM-Llama3-V-2_5-int4. Task: Visual Question Answering. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: segmentation-models. Task: Semantic Segmentation. Action:  assign a semantic label to each pixel in the input image, effectively segmenting the image into meaningful regions or objects.. Input Data: Video/Images. Framework: Tensorflow. Institution: n.a', 'Model: depth-anything/Depth-Anything-V2-Small-hf. Task: Depth Estimation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: tencent/DepthCrafter. Task: Depth Estimation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Intel/dpt-hybrid-midas. Task: Depth Estimation. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Ultralytics/YOLOv8. Task: Object Detection. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: facebook/detr-resnet-50. Task: Object Detection. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: microsoft/table-transformer-detection. Task: Object Detection. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: stable-diffusion-v1-5/stable-diffusion-v1-5. Task: Text-to-Image. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: black-forest-labs/FLUX.1-dev. Task: Text-to-Image. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: stabilityai/stable-diffusion-xl-refiner-1.0. Task: Image-to-image. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: lllyasviel/sd-controlnet-canny. Task: Image-to-image. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: stable-diffusion-v1-5/stable-diffusion-inpainting. Task: Image-to-image. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Salesforce/blip-image-captioning-base. Task: Image-to-text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: microsoft/trocr-base-handwritten. Task: Image-to-text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: IAMJB/chexpert-mimic-cxr-findings-baseline. Task: Image-to-text. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: stabilityai/stable-video-diffusion-img2vid-xt. Task: Image-to-video. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Wan-AI/Wan2.1-I2V-14B-480P-Diffusers. Task: Image-to-video. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: tencent/HunyuanVideo-I2V. Task: Image-to-video. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: xclid. Task: Video Classification . Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: videomaev2. Task: Video Classification . Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: vivit-b. Task: Video Classification . Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: google/vit-base-patch16-224-in21k. Task: Image Feature Extraction. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: facebook/dinov2-base. Task: Image Feature Extraction. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: nvidia/MambaVision-S-1K. Task: Image Feature Extraction. Action: . Input Data: Video/Images. Framework: . Institution: ', 'Model: Nothing for Descriptive Analysis. Task: Descriptive analysis. Action: summarize and describe the input data using statistical techniques.. Input Data: Tabular data. Framework: ?. Institution: ', 'Model: ShapeNet. Task: Recommender systems. Action:  recommend relevant items (e.g., products, movies, music) to users based on their preferences and behaviors.. Input Data: Tabular data. Framework: Tensorflow. Institution: N.a', 'Model: Adversarial Autoencoders. Task: Recommender systems. Action:  recommend relevant items (e.g., products, movies, music) to users based on their preferences and behaviors.. Input Data: Tabular data. Framework: PyTorch. Institution: N.a', 'Model: Recommenders. Task: Recommender systems. Action:  recommend relevant items (e.g., products, movies, music) to users based on their preferences and behaviors.. Input Data: Tabular data. Framework: Tensorflow. Institution: N.a', 'Model: mildnet. Task: Recommender systems. Action:  recommend relevant items (e.g., products, movies, music) to users based on their preferences and behaviors.. Input Data: Tabular data. Framework: Tensorflow. Institution: gofynd', 'Model: PaddleRec. Task: Recommender systems. Action:  recommend relevant items (e.g., products, movies, music) to users based on their preferences and behaviors.. Input Data: Tabular data. Framework: Paddle. Institution: PaddlePaddle', 'Model: FuxiCTR. Task: Recommender systems. Action:  recommend relevant items (e.g., products, movies, music) to users based on their preferences and behaviors.. Input Data: Tabular data. Framework: PyTorch. Institution: Xue-p ai', 'Model: ibm-granite/granite-timeseries-ttm-r1. Task: Time Series Forecasting. Action: . Input Data: . Framework: . Institution: ', 'Model: Temporal Fusion Transformer (TFT). Task: Time Series Forecasting. Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: Tensorflow. Institution: N.a', 'Model: LSTNet. Task: Time Series Forecasting. Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: PyTorch. Institution: N.a', 'Model: darts. Task: Time Series Forecasting. Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: PyTorch. Institution: N.a', 'Model: gluon-ts. Task: Time Series Forecasting. Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: mxnet. Institution: awslab', 'Model: qlib. Task: Time Series Forecasting. Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: PyTorch. Institution: Microsoft ', 'Model: DCRNN. Task: Time Series Forecasting. Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: Tensorflow. Institution: liyaguang', 'Model: n-beats. Task: Time Series Forecasting. Action: . Input Data: . Framework: . Institution: ', 'Model: pytorch_geometric_temporal. Task: Time Series Forecasting. Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: PyTorch. Institution: Benedekrozemberczki', 'Model: tapas-base-finetuned-wtq. Task: Table Question Answering . Action: provide answers to questions based on the input data.. Input Data: Tabular data. Framework: . Institution: google', 'Model: tapas-large-finetuned-wtq. Task: Table Question Answering . Action: provide answers to questions based on the input data.. Input Data: Tabular data. Framework: . Institution: google', 'Model: tapex-large-finetuned-wtq. Task: Table Question Answering . Action: provide answers to questions based on the input data.. Input Data: Tabular data. Framework: . Institution: microsoft', 'Model: Efficient Estimation of Word Representations in Vector Space. Task: Recommender systems. Action: predict/recommend future values based on historical patterns.. Input Data: Tabular data. Framework: Tensorflow. Institution: N.a', 'Model: DeepCTR. Task: Forecast (time series forcast). Action: predict future values based on historical patterns.. Input Data: Tabular data. Framework: Tensorflow. Institution: N.a', 'Model: TabNet. Task: Tabular Classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: XGBoost. Task: Tabular Classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: CatBoost. Task: Tabular Classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: LightGBM. Task: Tabular Regression. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: Narrativaai/bloom-560m-finetuned-totto-table-to-text. Task: Table to Text. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: Yale-LILY/reastap-large-finetuned-logicnlg. Task: Table to Text. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: Multimodal Toolkit. Task: Tabular to Text. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8. Task: Reinforcement Learning. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B. Task: Reinforcement Learning. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: ThomasSimonini/ppo-AntBulletEnv-v0. Task: Reinforcement Learning. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: lerobot/pi0. Task: Robotics. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: nvidia/GR00T-N1-2B. Task: Robotics. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: physical-intelligence/fast. Task: Robotics. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: ecmwf/aifs-single-1.0. Task: Graph Machine Learning. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: ibm-research/materials.pos-egnn. Task: Graph Machine Learning. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: clefourrier/graphormer-base-pcqm4mv2. Task: Graph Machine Learning. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: bztest/xlm-roberta-base-finetuned-panx-de. Task: Token Classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: FacebookAI/xlm-roberta-large-finetuned-conll03-english. Task: Token Classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: dslim/bert-base-NER. Task: Token Classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: facebook/bart-large-mnli. Task: Zero-shot classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli. Task: Zero-shot classification. Action: . Input Data: Tabular data. Framework: . Institution: ', 'Model: joeddav/bart-large-mnli-yahoo-answers. Task: Zero-shot classification. Action: . Input Data: Tabular data. Framework: . Institution: ']\n"
     ]
    }
   ],
   "source": [
    "collection = client.get_collection(name='model_data')\n",
    "data = collection.get()\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
